version: '3'
# Set DOCKERFILE=Dockerfile.cpu to build a CPU-only image
services:
  data_handler:
    build:
      context: .
      dockerfile: ${DOCKERFILE:-Dockerfile}
    # Run the full data handler implementation
    command: gunicorn -w 1 -b 0.0.0.0:8000 bot.data_handler:api_app
    runtime: ${RUNTIME:-nvidia}
    ports:
      - "8000:8000"
    env_file: .env
    environment:
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/nvvm/lib64
      - PYTHONPATH=/app
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/ping"]
      interval: 5s
      timeout: 2s
      retries: 12
      start_period: 5s
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./cache:/app/cache
      - ./config.json:/app/config.json
    shm_size: '8gb'
    networks:
      - trading_bot_default
  model_builder:
    build:
      context: .
      dockerfile: ${DOCKERFILE:-Dockerfile}
    # Run the full model builder implementation
    command: gunicorn -w 1 -b 0.0.0.0:8001 bot.model_builder:api_app
    runtime: ${RUNTIME:-nvidia}
    ports:
      - "8001:8001"
    environment:
      - TF_CPP_MIN_LOG_LEVEL=3
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/nvvm/lib64
      - PYTHONPATH=/app
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/ping"]
      interval: 5s
      timeout: 2s
      retries: 12
      start_period: 5s
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./cache:/app/cache
      - ./config.json:/app/config.json
    shm_size: '8gb'
    networks:
      - trading_bot_default
  trade_manager:
    build:
      context: .
      dockerfile: ${DOCKERFILE:-Dockerfile}
    # Use the ASGI wrapper exported by trade_manager so UvicornWorker can
    # serve the Flask app correctly.
    command: gunicorn -w 1 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8002 bot.trade_manager:asgi_app
    runtime: ${RUNTIME:-nvidia}
    ports:
      - "8002:8002"
    env_file: .env
    environment:
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/nvvm/lib64
      - PYTHONPATH=/app
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/ready"]
      interval: 5s
      timeout: 2s
      retries: 12
      start_period: 5s
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./cache:/app/cache
      - ./config.json:/app/config.json
    shm_size: '8gb'
    networks:
      - trading_bot_default
  trading_bot:
    build:
      context: .
      dockerfile: ${DOCKERFILE:-Dockerfile}
    # container_name: trading_bot
    command: python -m bot.trading_bot
    runtime: ${RUNTIME:-nvidia}
    depends_on:
      data_handler:
        condition: service_healthy
      model_builder:
        condition: service_healthy
      trade_manager:
        condition: service_healthy
    environment:
      - DATA_HANDLER_URL=http://data_handler:8000
      - MODEL_BUILDER_URL=http://model_builder:8001
      - TRADE_MANAGER_URL=http://trade_manager:8002
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - NVIDIA_DRIVER_CAPABILITIES=${NVIDIA_DRIVER_CAPABILITIES:-compute,utility}
      - PYTHONUNBUFFERED=1
      - PYTHONPATH=/app
      - BYBIT_API_KEY=${BYBIT_API_KEY}
      - BYBIT_API_SECRET=${BYBIT_API_SECRET}
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - TELEGRAM_CHAT_ID=${TELEGRAM_CHAT_ID}
      - CACHE_DIR=/app/cache
      - LOG_DIR=/app/logs
      - MODEL_SAVE_PATH=/app/models
      - CONFIG_PATH=/app/config.json
      - SERVICE_CHECK_RETRIES=${SERVICE_CHECK_RETRIES}
      - SERVICE_CHECK_DELAY=${SERVICE_CHECK_DELAY}
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./cache:/app/cache
      - ./config.json:/app/config.json
    networks:
      - trading_bot_default
    restart: unless-stopped
    shm_size: '8gb'
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
networks:
  trading_bot_default:
    driver: bridge
