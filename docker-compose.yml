# Set DOCKERFILE=Dockerfile.cpu to build a CPU-only image
services:
  gptoss:
    image: ghcr.io/openaccess-ai-collective/gpt-oss:cpu-latest
    ports:
      - "8003:8000"
    volumes:
      - gptoss_workspace:/workspace
    environment:
      - TRANSFORMERS_CACHE=${TRANSFORMERS_CACHE}
    # Ensure the GPT-OSS API is ready before other services start
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health"]
      interval: 5s
      timeout: 2s
      retries: 12
      start_period: 5s
    networks:
      - gptoss_net
  data_handler:
    build:
      context: .
      dockerfile: ${DOCKERFILE:-Dockerfile}
    # Run the full data handler implementation
    command: gunicorn -w 1 -b 0.0.0.0:8000 bot.data_handler:api_app
    runtime: ${RUNTIME:-runc}
    ports:
      - "8000:8000"
    env_file: .env
    environment:
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/nvvm/lib64
      - PYTHONPATH=/app
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/ping"]
      interval: 5s
      timeout: 2s
      retries: 12
      start_period: 5s
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./cache:/app/cache
      - ./config.json:/app/config.json
    shm_size: '8gb'
    networks:
      - trading_bot_default
  model_builder:
    build:
      context: .
      dockerfile: ${DOCKERFILE:-Dockerfile}
    # Run the full model builder implementation
    command: gunicorn -w 1 -b 0.0.0.0:8001 bot.model_builder:api_app
    runtime: ${RUNTIME:-runc}
    ports:
      - "8001:8001"
    environment:
      - TF_CPP_MIN_LOG_LEVEL=3
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/nvvm/lib64
      - PYTHONPATH=/app
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/ping"]
      interval: 5s
      timeout: 2s
      retries: 12
      start_period: 5s
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./cache:/app/cache
      - ./config.json:/app/config.json
    shm_size: '8gb'
    networks:
      - trading_bot_default
  trade_manager:
    build:
      context: .
      dockerfile: ${DOCKERFILE:-Dockerfile}
    # Use the ASGI wrapper exported by trade_manager so UvicornWorker can
    # serve the Flask app correctly.
    command: gunicorn -w 1 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8002 bot.trade_manager:asgi_app
    runtime: ${RUNTIME:-runc}
    ports:
      - "8002:8002"
    env_file: .env
    environment:
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/nvvm/lib64
      - PYTHONPATH=/app
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/ready"]
      interval: 5s
      timeout: 2s
      retries: 12
      start_period: 5s
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./cache:/app/cache
      - ./config.json:/app/config.json
    shm_size: '8gb'
    networks:
      - trading_bot_default
  bot:
    build:
      context: .
      dockerfile: ${DOCKERFILE:-Dockerfile}
    # container_name: trading_bot
    command: python -m bot.trading_bot
    runtime: ${RUNTIME:-runc}
    depends_on:
      # Wait for GPT-OSS to become healthy
      gptoss:
        condition: service_healthy
      data_handler:
        condition: service_healthy
      model_builder:
        condition: service_healthy
      trade_manager:
        condition: service_healthy
      gptoss_check:
        condition: service_completed_successfully
    environment:
      - DATA_HANDLER_URL=${DATA_HANDLER_URL}
      - MODEL_BUILDER_URL=${MODEL_BUILDER_URL}
      - TRADE_MANAGER_URL=${TRADE_MANAGER_URL}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - NVIDIA_DRIVER_CAPABILITIES=${NVIDIA_DRIVER_CAPABILITIES:-compute,utility}
      - PYTHONUNBUFFERED=${PYTHONUNBUFFERED}
      - PYTHONPATH=${PYTHONPATH}
      - BYBIT_API_KEY=${BYBIT_API_KEY}
      - BYBIT_API_SECRET=${BYBIT_API_SECRET}
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - TELEGRAM_CHAT_ID=${TELEGRAM_CHAT_ID}
      - CACHE_DIR=${CACHE_DIR}
      - LOG_DIR=${LOG_DIR}
      - MODEL_SAVE_PATH=${MODEL_SAVE_PATH}
      - CONFIG_PATH=${CONFIG_PATH}
      - SERVICE_CHECK_RETRIES=${SERVICE_CHECK_RETRIES}
      - SERVICE_CHECK_DELAY=${SERVICE_CHECK_DELAY}
      - GPT_OSS_API=${GPT_OSS_API}
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./cache:/app/cache
      - ./config.json:/app/config.json
    networks:
      - trading_bot_default
      - gptoss_net
    restart: unless-stopped
    shm_size: '8gb'
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
  gptoss_check:
    image: averinaleks/myapp:latest
    command: python3 gptoss_check/check_code.py
    working_dir: /workspace
    volumes:
      - .:/workspace
    environment:
      PYTHONUNBUFFERED: "1"
      GPT_OSS_API: http://gptoss:8000
    depends_on:
      - gptoss
    networks:
      - gptoss_net
networks:
  trading_bot_default:
    driver: bridge
  gptoss_net:
    name: gptoss_net
    driver: bridge

volumes:
  gptoss_workspace:
