services:
  gptoss:
    build:
      context: .
      dockerfile: Dockerfile.gptoss
    environment:
      TRANSFORMERS_CACHE: /workspace
      VLLM_LOGGING_LEVEL: DEBUG
    ports:
      - "8003:8000"
    volumes:
      - gptoss_workspace:/workspace
      - ./logs:/app/logs
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 120s
    networks:
      - gptoss_net
  data_handler:
    image: ${APP_IMAGE:-trading_bot:latest}
    # Run the full data handler implementation
    command:
      - gunicorn
      - -w
      - "1"
      - -b
      - 0.0.0.0:8000
      - --access-logfile
      - /app/logs/data_handler.access.log
      - --error-logfile
      - /app/logs/data_handler.error.log
      - bot.data_handler:api_app
    ports:
      - "8000:8000"
    env_file: .env
    environment:
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/nvvm/lib64
      - PYTHONPATH=/app
    healthcheck:
      test: ["CMD", "curl", "--netrc-file", "/dev/null", "-f", "http://localhost:8000/health"]
      interval: 5s
      timeout: 2s
      retries: 12
      start_period: 5s
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./cache:/app/cache
      - ./config.json:/app/config.json
    shm_size: '8gb'
    networks:
      - trading_bot_default
  model_builder:
    image: ${APP_IMAGE:-trading_bot:latest}
    # Run the full model builder implementation
    command:
      - gunicorn
      - -w
      - "1"
      - -b
      - 0.0.0.0:8001
      - --access-logfile
      - /app/logs/model_builder.access.log
      - --error-logfile
      - /app/logs/model_builder.error.log
      - bot.model_builder:api_app
    ports:
      - "8001:8001"
    env_file: .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - TF_CPP_MIN_LOG_LEVEL=3
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/nvvm/lib64
      - PYTHONPATH=/app
    healthcheck:
      test: ["CMD", "curl", "--netrc-file", "/dev/null", "-f", "http://localhost:8001/ping"]
      interval: 5s
      timeout: 2s
      retries: 12
      start_period: 5s
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./cache:/app/cache
      - ./config.json:/app/config.json
    shm_size: '8gb'
    networks:
      - trading_bot_default
  trade_manager:
    image: ${APP_IMAGE:-trading_bot:latest}
    # Use the ASGI wrapper exported by trade_manager so UvicornWorker can
    # serve the Flask app correctly.
    command:
      - gunicorn
      - -w
      - "1"
      - -k
      - uvicorn.workers.UvicornWorker
      - -b
      - 0.0.0.0:8002
      - --access-logfile
      - /app/logs/trade_manager.access.log
      - --error-logfile
      - /app/logs/trade_manager.error.log
      - bot.trade_manager:asgi_app
    ports:
      - "8002:8002"
    env_file: .env
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/nvvm/lib64
      - PYTHONPATH=/app
    healthcheck:
      test: ["CMD", "curl", "--netrc-file", "/dev/null", "-f", "http://localhost:8002/ready"]
      interval: 5s
      timeout: 2s
      retries: 12
      start_period: 5s
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./cache:/app/cache
      - ./config.json:/app/config.json
    shm_size: '8gb'
    networks:
      - trading_bot_default
  bot:
    image: ${APP_IMAGE:-trading_bot:latest}
    build:
      context: .
      dockerfile: ${DOCKERFILE:-Dockerfile}
    # container_name: trading_bot
    command: python -m bot.trading_bot
    depends_on:
      # Wait for GPT-OSS to become healthy
      gptoss:
        condition: service_healthy
      data_handler:
        condition: service_healthy
      model_builder:
        condition: service_healthy
      trade_manager:
        condition: service_healthy
      gptoss_check:
        condition: service_completed_successfully
    env_file: .env
    environment:
      - DATA_HANDLER_URL=${DATA_HANDLER_URL}
      - MODEL_BUILDER_URL=${MODEL_BUILDER_URL}
      - TRADE_MANAGER_URL=${TRADE_MANAGER_URL}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - NVIDIA_DRIVER_CAPABILITIES=${NVIDIA_DRIVER_CAPABILITIES:-compute,utility}
      - PYTHONUNBUFFERED=${PYTHONUNBUFFERED}
      - PYTHONPATH=${PYTHONPATH}
      - BYBIT_API_KEY=${BYBIT_API_KEY}
      - BYBIT_API_SECRET=${BYBIT_API_SECRET}
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - TELEGRAM_CHAT_ID=${TELEGRAM_CHAT_ID}
      - CACHE_DIR=${CACHE_DIR}
      - LOG_DIR=${LOG_DIR}
      - LOG_LEVEL=${LOG_LEVEL}
      - MODEL_SAVE_PATH=${MODEL_SAVE_PATH}
      - CONFIG_PATH=${CONFIG_PATH}
      - SERVICE_CHECK_RETRIES=${SERVICE_CHECK_RETRIES}
      - SERVICE_CHECK_DELAY=${SERVICE_CHECK_DELAY}
      - GPT_OSS_API=${GPT_OSS_API}
    volumes:
      - ./logs:/app/logs
      - ./models:/app/models
      - ./cache:/app/cache
      - ./config.json:/app/config.json
    networks:
      - trading_bot_default
      - gptoss_net
    restart: unless-stopped
    shm_size: '8gb'
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
  gptoss_check:
    image: python:3.12-slim
    entrypoint:
      - python
      - -m
      - gptoss_check
    working_dir: /workspace
    volumes:
      - .:/workspace
    environment:
      PYTHONUNBUFFERED: "1"
      GPT_OSS_API: http://gptoss:8000
    depends_on:
      gptoss:
        condition: service_healthy
    networks:
      - gptoss_net
networks:
  trading_bot_default:
    driver: bridge
  gptoss_net:
    name: gptoss_net
    driver: bridge

volumes:
  gptoss_workspace:
